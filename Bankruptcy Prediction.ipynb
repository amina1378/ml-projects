{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\amina\\OneDrive\\Desktop\\companies-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "0          1                                           0.370594          \n",
       "1          1                                           0.464291          \n",
       "2          1                                           0.426071          \n",
       "3          1                                           0.399844          \n",
       "4          1                                           0.465022          \n",
       "\n",
       "    ROA(A) before interest and % after tax  \\\n",
       "0                                 0.424389   \n",
       "1                                 0.538214   \n",
       "2                                 0.499019   \n",
       "3                                 0.451265   \n",
       "4                                 0.538432   \n",
       "\n",
       "    ROA(B) before interest and depreciation after tax  \\\n",
       "0                                           0.405750    \n",
       "1                                           0.516730    \n",
       "2                                           0.472295    \n",
       "3                                           0.457733    \n",
       "4                                           0.522298    \n",
       "\n",
       "    Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                 0.601457                      0.601457   \n",
       "1                 0.610235                      0.610235   \n",
       "2                 0.601450                      0.601364   \n",
       "3                 0.583541                      0.583541   \n",
       "4                 0.598783                      0.598783   \n",
       "\n",
       "    Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                0.998969                    0.796887   \n",
       "1                0.998946                    0.797380   \n",
       "2                0.998857                    0.796403   \n",
       "3                0.998700                    0.796967   \n",
       "4                0.998973                    0.797366   \n",
       "\n",
       "    After-tax net Interest Rate   Non-industry income and expenditure/revenue  \\\n",
       "0                      0.808809                                      0.302646   \n",
       "1                      0.809301                                      0.303556   \n",
       "2                      0.808388                                      0.302035   \n",
       "3                      0.808966                                      0.303350   \n",
       "4                      0.809304                                      0.303475   \n",
       "\n",
       "   ...   Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0  ...                     0.716845                    0.009219   \n",
       "1  ...                     0.795297                    0.008323   \n",
       "2  ...                     0.774670                    0.040003   \n",
       "3  ...                     0.739555                    0.003252   \n",
       "4  ...                     0.795016                    0.003878   \n",
       "\n",
       "    No-credit Interval   Gross Profit to Sales  \\\n",
       "0             0.622879                0.601453   \n",
       "1             0.623652                0.610237   \n",
       "2             0.623841                0.601449   \n",
       "3             0.622929                0.583538   \n",
       "4             0.623521                0.598782   \n",
       "\n",
       "    Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                             0.827890              0.290202   \n",
       "1                             0.839969              0.283846   \n",
       "2                             0.836774              0.290189   \n",
       "3                             0.834697              0.281721   \n",
       "4                             0.839973              0.278514   \n",
       "\n",
       "    Degree of Financial Leverage (DFL)  \\\n",
       "0                             0.026601   \n",
       "1                             0.264577   \n",
       "2                             0.026555   \n",
       "3                             0.026697   \n",
       "4                             0.024752   \n",
       "\n",
       "    Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "0                                           0.564050                   1   \n",
       "1                                           0.570175                   1   \n",
       "2                                           0.563706                   1   \n",
       "3                                           0.564663                   1   \n",
       "4                                           0.575617                   1   \n",
       "\n",
       "    Equity to Liability  \n",
       "0              0.016469  \n",
       "1              0.020794  \n",
       "2              0.016474  \n",
       "3              0.023982  \n",
       "4              0.035490  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bankrupt?                                                     int64\n",
       " ROA(C) before interest and depreciation before interest    float64\n",
       " ROA(A) before interest and % after tax                     float64\n",
       " ROA(B) before interest and depreciation after tax          float64\n",
       " Operating Gross Margin                                     float64\n",
       "                                                             ...   \n",
       " Liability to Equity                                        float64\n",
       " Degree of Financial Leverage (DFL)                         float64\n",
       " Interest Coverage Ratio (Interest expense to EBIT)         float64\n",
       " Net Income Flag                                              int64\n",
       " Equity to Liability                                        float64\n",
       "Length: 96, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Net Income Flag\n"
     ]
    }
   ],
   "source": [
    "for i in list(data.columns):\n",
    "    if data[i].std() == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(' Net Income Flag', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6599\n",
       "1     220\n",
       "Name: Bankrupt?, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Bankrupt?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Bankrupt?', axis=1).values\n",
    "y = data['Bankrupt?'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "pca = PCA(n_components=0.9).fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5455, 45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegressionCV(class_weight='balanced', scoring='precision', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(class_weight='balanced', cv=5, scoring='precision')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1152,  168],\n",
       "       [  11,   33]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, log_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200, n_jobs=-1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth':[2,4,6,8],\n",
    "    'min_samples_split':[2,4,6,8],\n",
    "    'min_samples_leaf':[1,3,5,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuner = RandomizedSearchCV(rf_model, params, n_iter=20, scoring='recall', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                                    n_estimators=200,\n",
       "                                                    n_jobs=-1),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'max_depth': [2, 4, 6, 8],\n",
       "                                        'min_samples_leaf': [1, 3, 5, 7],\n",
       "                                        'min_samples_split': [2, 4, 6, 8]},\n",
       "                   scoring='recall')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tuner.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1180,  140],\n",
       "       [  11,   33]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rf_tuner.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=(X_train.shape[1],))\n",
    "x = keras.layers.Dense(32, activation='relu')(input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.models.Model(input, output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(0.0001), metrics=[keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = np.unique(np.unique(y))\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_dict = {}\n",
    "for i in classes:\n",
    "    class_dict[i] = class_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(keras.callbacks.Callback):\n",
    "    best = 0\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('val_recall') > self.best:\n",
    "            self.best = logs.get('val_recall')\n",
    "            model.save('bestmodel.hdf5')\n",
    "        print(f'\\nbest recall is : {self.best}')\n",
    "mc = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "callbacks = [ReduceLROnPlateau(patience=2), mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/137 [=======================>......] - ETA: 0s - loss: 0.5507 - recall: 0.8783\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.5426 - recall: 0.8889 - val_loss: 0.9492 - val_recall: 0.8750\n",
      "Epoch 2/100\n",
      "115/137 [========================>.....] - ETA: 0s - loss: 0.5282 - recall: 0.8760\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.5209 - recall: 0.8889 - val_loss: 0.7855 - val_recall: 0.8125\n",
      "Epoch 3/100\n",
      "116/137 [========================>.....] - ETA: 0s - loss: 0.4894 - recall: 0.9055\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4858 - recall: 0.9097 - val_loss: 0.7327 - val_recall: 0.8125\n",
      "Epoch 4/100\n",
      "113/137 [=======================>......] - ETA: 0s - loss: 0.4667 - recall: 0.9009\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4718 - recall: 0.9028 - val_loss: 0.6930 - val_recall: 0.8125\n",
      "Epoch 5/100\n",
      "111/137 [=======================>......] - ETA: 0s - loss: 0.4553 - recall: 0.9035\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4481 - recall: 0.9236 - val_loss: 0.6769 - val_recall: 0.8125\n",
      "Epoch 6/100\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.4284 - recall: 0.9137\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4314 - recall: 0.9097 - val_loss: 0.6494 - val_recall: 0.8125\n",
      "Epoch 7/100\n",
      "111/137 [=======================>......] - ETA: 0s - loss: 0.4174 - recall: 0.9224\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4222 - recall: 0.9167 - val_loss: 0.6250 - val_recall: 0.7812\n",
      "Epoch 8/100\n",
      "122/137 [=========================>....] - ETA: 0s - loss: 0.4096 - recall: 0.9403\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.4106 - recall: 0.9306 - val_loss: 0.5984 - val_recall: 0.7812\n",
      "Epoch 9/100\n",
      "115/137 [========================>.....] - ETA: 0s - loss: 0.3867 - recall: 0.9397\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3897 - recall: 0.9306 - val_loss: 0.5595 - val_recall: 0.7500\n",
      "Epoch 10/100\n",
      "112/137 [=======================>......] - ETA: 0s - loss: 0.3904 - recall: 0.8909\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3958 - recall: 0.8958 - val_loss: 0.5425 - val_recall: 0.7500\n",
      "Epoch 11/100\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.3922 - recall: 0.9137\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3872 - recall: 0.9167 - val_loss: 0.5301 - val_recall: 0.7812\n",
      "Epoch 12/100\n",
      "113/137 [=======================>......] - ETA: 0s - loss: 0.3742 - recall: 0.9153\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3810 - recall: 0.9167 - val_loss: 0.5160 - val_recall: 0.7500\n",
      "Epoch 13/100\n",
      "108/137 [======================>.......] - ETA: 0s - loss: 0.3724 - recall: 0.9151\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3737 - recall: 0.9097 - val_loss: 0.4997 - val_recall: 0.7500\n",
      "Epoch 14/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3520 - recall: 0.9375\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3518 - recall: 0.9375 - val_loss: 0.5011 - val_recall: 0.7812\n",
      "Epoch 15/100\n",
      "117/137 [========================>.....] - ETA: 0s - loss: 0.3729 - recall: 0.8803\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3765 - recall: 0.8889 - val_loss: 0.4820 - val_recall: 0.7812\n",
      "Epoch 16/100\n",
      "106/137 [======================>.......] - ETA: 0s - loss: 0.3663 - recall: 0.9189\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3565 - recall: 0.9306 - val_loss: 0.4754 - val_recall: 0.7500\n",
      "Epoch 17/100\n",
      "101/137 [=====================>........] - ETA: 0s - loss: 0.3603 - recall: 0.9126\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3494 - recall: 0.9375 - val_loss: 0.4527 - val_recall: 0.6875\n",
      "Epoch 18/100\n",
      "108/137 [======================>.......] - ETA: 0s - loss: 0.3387 - recall: 0.9244\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3409 - recall: 0.9097 - val_loss: 0.4526 - val_recall: 0.7500\n",
      "Epoch 19/100\n",
      "112/137 [=======================>......] - ETA: 0s - loss: 0.3395 - recall: 0.9074\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3499 - recall: 0.9028 - val_loss: 0.4298 - val_recall: 0.6562\n",
      "Epoch 20/100\n",
      "105/137 [=====================>........] - ETA: 0s - loss: 0.3490 - recall: 0.9035\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3418 - recall: 0.9236 - val_loss: 0.4244 - val_recall: 0.6562\n",
      "Epoch 21/100\n",
      "105/137 [=====================>........] - ETA: 0s - loss: 0.3247 - recall: 0.9189\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3305 - recall: 0.9097 - val_loss: 0.4191 - val_recall: 0.6875\n",
      "Epoch 22/100\n",
      "106/137 [======================>.......] - ETA: 0s - loss: 0.3543 - recall: 0.8818\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3399 - recall: 0.9028 - val_loss: 0.4001 - val_recall: 0.6562\n",
      "Epoch 23/100\n",
      "107/137 [======================>.......] - ETA: 0s - loss: 0.3387 - recall: 0.9115\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3359 - recall: 0.9167 - val_loss: 0.4119 - val_recall: 0.6562\n",
      "Epoch 24/100\n",
      "103/137 [=====================>........] - ETA: 0s - loss: 0.3127 - recall: 0.9135\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3217 - recall: 0.9097 - val_loss: 0.4044 - val_recall: 0.6875\n",
      "Epoch 25/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3221 - recall: 0.9161\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3212 - recall: 0.9167 - val_loss: 0.4035 - val_recall: 0.6875\n",
      "Epoch 26/100\n",
      "107/137 [======================>.......] - ETA: 0s - loss: 0.3366 - recall: 0.9145\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3220 - recall: 0.9167 - val_loss: 0.3978 - val_recall: 0.6875\n",
      "Epoch 27/100\n",
      "106/137 [======================>.......] - ETA: 0s - loss: 0.3206 - recall: 0.9238\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3294 - recall: 0.9097 - val_loss: 0.3967 - val_recall: 0.6875\n",
      "Epoch 28/100\n",
      "125/137 [==========================>...] - ETA: 0s - loss: 0.3297 - recall: 0.8939\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3345 - recall: 0.8889 - val_loss: 0.3933 - val_recall: 0.6562\n",
      "Epoch 29/100\n",
      "112/137 [=======================>......] - ETA: 0s - loss: 0.3231 - recall: 0.9167\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3196 - recall: 0.9167 - val_loss: 0.4027 - val_recall: 0.6875\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3249 - recall: 0.8819\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3249 - recall: 0.8819 - val_loss: 0.3983 - val_recall: 0.6875\n",
      "Epoch 31/100\n",
      " 94/137 [===================>..........] - ETA: 0s - loss: 0.3132 - recall: 0.9327\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3325 - recall: 0.9097 - val_loss: 0.3967 - val_recall: 0.6875\n",
      "Epoch 32/100\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.3287 - recall: 0.8865\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3250 - recall: 0.8889 - val_loss: 0.3982 - val_recall: 0.6875\n",
      "Epoch 33/100\n",
      "104/137 [=====================>........] - ETA: 0s - loss: 0.3041 - recall: 0.9200\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3267 - recall: 0.9028 - val_loss: 0.3873 - val_recall: 0.6562\n",
      "Epoch 34/100\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.3290 - recall: 0.8741\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3311 - recall: 0.8750 - val_loss: 0.3853 - val_recall: 0.5938\n",
      "Epoch 35/100\n",
      "113/137 [=======================>......] - ETA: 0s - loss: 0.3152 - recall: 0.9211\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3150 - recall: 0.9236 - val_loss: 0.3887 - val_recall: 0.6562\n",
      "Epoch 36/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3256 - recall: 0.9028\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3255 - recall: 0.9028 - val_loss: 0.3886 - val_recall: 0.6562\n",
      "Epoch 37/100\n",
      "116/137 [========================>.....] - ETA: 0s - loss: 0.3180 - recall: 0.9231\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3230 - recall: 0.9097 - val_loss: 0.3877 - val_recall: 0.6562\n",
      "Epoch 38/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3258 - recall: 0.9058\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3284 - recall: 0.9028 - val_loss: 0.3853 - val_recall: 0.6250\n",
      "Epoch 39/100\n",
      "119/137 [=========================>....] - ETA: 0s - loss: 0.3230 - recall: 0.9174\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3223 - recall: 0.9236 - val_loss: 0.4014 - val_recall: 0.6875\n",
      "Epoch 40/100\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.3279 - recall: 0.9220\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3265 - recall: 0.9236 - val_loss: 0.3847 - val_recall: 0.5938\n",
      "Epoch 41/100\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.3216 - recall: 0.9167\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3270 - recall: 0.9097 - val_loss: 0.3976 - val_recall: 0.6875\n",
      "Epoch 42/100\n",
      "125/137 [==========================>...] - ETA: 0s - loss: 0.3329 - recall: 0.8872\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3278 - recall: 0.8889 - val_loss: 0.3894 - val_recall: 0.6562\n",
      "Epoch 43/100\n",
      " 98/137 [====================>.........] - ETA: 0s - loss: 0.3302 - recall: 0.9358  \n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3250 - recall: 0.9236 - val_loss: 0.3979 - val_recall: 0.6875\n",
      "Epoch 44/100\n",
      "118/137 [========================>.....] - ETA: 0s - loss: 0.3141 - recall: 0.9113\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3135 - recall: 0.9236 - val_loss: 0.3903 - val_recall: 0.6562\n",
      "Epoch 45/100\n",
      "108/137 [======================>.......] - ETA: 0s - loss: 0.3258 - recall: 0.9018\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3214 - recall: 0.9097 - val_loss: 0.3924 - val_recall: 0.6875\n",
      "Epoch 46/100\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.3467 - recall: 0.8705\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3432 - recall: 0.8750 - val_loss: 0.3906 - val_recall: 0.6562\n",
      "Epoch 47/100\n",
      "109/137 [======================>.......] - ETA: 0s - loss: 0.3136 - recall: 0.9381\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3201 - recall: 0.9306 - val_loss: 0.3941 - val_recall: 0.6562\n",
      "Epoch 48/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3202 - recall: 0.9044\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3207 - recall: 0.9097 - val_loss: 0.3909 - val_recall: 0.6562\n",
      "Epoch 49/100\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.3199 - recall: 0.9242\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3224 - recall: 0.9167 - val_loss: 0.3964 - val_recall: 0.6875\n",
      "Epoch 50/100\n",
      "121/137 [=========================>....] - ETA: 0s - loss: 0.3286 - recall: 0.8810\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3251 - recall: 0.8889 - val_loss: 0.3959 - val_recall: 0.6562\n",
      "Epoch 51/100\n",
      "118/137 [========================>.....] - ETA: 0s - loss: 0.3102 - recall: 0.9274\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3175 - recall: 0.9097 - val_loss: 0.3967 - val_recall: 0.6875\n",
      "Epoch 52/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3300 - recall: 0.8889\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3330 - recall: 0.8889 - val_loss: 0.3893 - val_recall: 0.6250\n",
      "Epoch 53/100\n",
      "113/137 [=======================>......] - ETA: 0s - loss: 0.3215 - recall: 0.8992\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3324 - recall: 0.8958 - val_loss: 0.3945 - val_recall: 0.6875\n",
      "Epoch 54/100\n",
      "121/137 [=========================>....] - ETA: 0s - loss: 0.3067 - recall: 0.9187\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3128 - recall: 0.9167 - val_loss: 0.3888 - val_recall: 0.6250\n",
      "Epoch 55/100\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.3452 - recall: 0.8657\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3387 - recall: 0.8750 - val_loss: 0.3916 - val_recall: 0.6562\n",
      "Epoch 56/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3337 - recall: 0.8731\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3324 - recall: 0.8819 - val_loss: 0.3875 - val_recall: 0.6562\n",
      "Epoch 57/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3372 - recall: 0.8913\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3362 - recall: 0.8958 - val_loss: 0.4000 - val_recall: 0.6875\n",
      "Epoch 58/100\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.3312 - recall: 0.8978\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3312 - recall: 0.9028 - val_loss: 0.3948 - val_recall: 0.6562\n",
      "Epoch 59/100\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.3191 - recall: 0.9275\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3166 - recall: 0.9306 - val_loss: 0.3964 - val_recall: 0.6875\n",
      "Epoch 60/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3280 - recall: 0.8986\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3260 - recall: 0.9028 - val_loss: 0.3883 - val_recall: 0.6562\n",
      "Epoch 61/100\n",
      "128/137 [===========================>..] - ETA: 0s - loss: 0.3076 - recall: 0.9323\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3099 - recall: 0.9306 - val_loss: 0.3895 - val_recall: 0.6562\n",
      "Epoch 62/100\n",
      "127/137 [==========================>...] - ETA: 0s - loss: 0.3235 - recall: 0.9333\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3224 - recall: 0.9306 - val_loss: 0.3906 - val_recall: 0.6562\n",
      "Epoch 63/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3037 - recall: 0.9403\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3098 - recall: 0.9306 - val_loss: 0.3976 - val_recall: 0.7188\n",
      "Epoch 64/100\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.3388 - recall: 0.8865\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3341 - recall: 0.8889 - val_loss: 0.4027 - val_recall: 0.7188\n",
      "Epoch 65/100\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.3149 - recall: 0.9296\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3184 - recall: 0.9236 - val_loss: 0.3979 - val_recall: 0.6875\n",
      "Epoch 66/100\n",
      " 97/137 [====================>.........] - ETA: 0s - loss: 0.3324 - recall: 0.8710  \n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3251 - recall: 0.9028 - val_loss: 0.3997 - val_recall: 0.6875\n",
      "Epoch 67/100\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.3234 - recall: 0.9065\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3221 - recall: 0.9097 - val_loss: 0.3935 - val_recall: 0.6875\n",
      "Epoch 68/100\n",
      "125/137 [==========================>...] - ETA: 0s - loss: 0.3302 - recall: 0.8864\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3268 - recall: 0.8889 - val_loss: 0.3860 - val_recall: 0.5938\n",
      "Epoch 69/100\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.3240 - recall: 0.8955\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3200 - recall: 0.9028 - val_loss: 0.3932 - val_recall: 0.6562\n",
      "Epoch 70/100\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.3355 - recall: 0.8881\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3348 - recall: 0.8889 - val_loss: 0.3931 - val_recall: 0.6562\n",
      "Epoch 71/100\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.3165 - recall: 0.9291\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3174 - recall: 0.9306 - val_loss: 0.3928 - val_recall: 0.6250\n",
      "Epoch 72/100\n",
      "127/137 [==========================>...] - ETA: 0s - loss: 0.3314 - recall: 0.8881\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3280 - recall: 0.8958 - val_loss: 0.3934 - val_recall: 0.6562\n",
      "Epoch 73/100\n",
      "113/137 [=======================>......] - ETA: 0s - loss: 0.3252 - recall: 0.9048\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3216 - recall: 0.9028 - val_loss: 0.3968 - val_recall: 0.6562\n",
      "Epoch 74/100\n",
      "105/137 [=====================>........] - ETA: 0s - loss: 0.3118 - recall: 0.9444\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3191 - recall: 0.9306 - val_loss: 0.3996 - val_recall: 0.6875\n",
      "Epoch 75/100\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.3334 - recall: 0.9301\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3306 - recall: 0.9306 - val_loss: 0.4025 - val_recall: 0.6875\n",
      "Epoch 76/100\n",
      "107/137 [======================>.......] - ETA: 0s - loss: 0.3199 - recall: 0.9217\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3227 - recall: 0.9028 - val_loss: 0.3944 - val_recall: 0.6562\n",
      "Epoch 77/100\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.3218 - recall: 0.9000\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3209 - recall: 0.9028 - val_loss: 0.4032 - val_recall: 0.6875\n",
      "Epoch 78/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3261 - recall: 0.9028\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3261 - recall: 0.9028 - val_loss: 0.3978 - val_recall: 0.6875\n",
      "Epoch 79/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3195 - recall: 0.8958\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3195 - recall: 0.8958 - val_loss: 0.3901 - val_recall: 0.6250\n",
      "Epoch 80/100\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.3206 - recall: 0.9281\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3184 - recall: 0.9306 - val_loss: 0.4034 - val_recall: 0.6875\n",
      "Epoch 81/100\n",
      " 96/137 [====================>.........] - ETA: 0s - loss: 0.3234 - recall: 0.8942  \n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3346 - recall: 0.8750 - val_loss: 0.4039 - val_recall: 0.6875\n",
      "Epoch 82/100\n",
      "113/137 [=======================>......] - ETA: 0s - loss: 0.3131 - recall: 0.9328\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3176 - recall: 0.9306 - val_loss: 0.3950 - val_recall: 0.6875\n",
      "Epoch 83/100\n",
      "128/137 [===========================>..] - ETA: 0s - loss: 0.3292 - recall: 0.8978\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3250 - recall: 0.9028 - val_loss: 0.4043 - val_recall: 0.6875\n",
      "Epoch 84/100\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.3175 - recall: 0.9104\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 1ms/step - loss: 0.3218 - recall: 0.8958 - val_loss: 0.3988 - val_recall: 0.6875\n",
      "Epoch 85/100\n",
      "122/137 [=========================>....] - ETA: 0s - loss: 0.3308 - recall: 0.9040\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3249 - recall: 0.9097 - val_loss: 0.3910 - val_recall: 0.6562\n",
      "Epoch 86/100\n",
      "118/137 [========================>.....] - ETA: 0s - loss: 0.3209 - recall: 0.8917\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3275 - recall: 0.8889 - val_loss: 0.3975 - val_recall: 0.7188\n",
      "Epoch 87/100\n",
      "115/137 [========================>.....] - ETA: 0s - loss: 0.3306 - recall: 0.9127\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3156 - recall: 0.9236 - val_loss: 0.3876 - val_recall: 0.5938\n",
      "Epoch 88/100\n",
      "124/137 [==========================>...] - ETA: 0s - loss: 0.3408 - recall: 0.8846\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3430 - recall: 0.8819 - val_loss: 0.3986 - val_recall: 0.6875\n",
      "Epoch 89/100\n",
      "111/137 [=======================>......] - ETA: 0s - loss: 0.3214 - recall: 0.9147\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3105 - recall: 0.9236 - val_loss: 0.3928 - val_recall: 0.6562\n",
      "Epoch 90/100\n",
      "103/137 [=====================>........] - ETA: 0s - loss: 0.3300 - recall: 0.9009   - ETA: 0s - loss: 0.3178 - recall: 0.93\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3293 - recall: 0.9028 - val_loss: 0.3972 - val_recall: 0.6875\n",
      "Epoch 91/100\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.3197 - recall: 0.9124\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3192 - recall: 0.9167 - val_loss: 0.4046 - val_recall: 0.6875\n",
      "Epoch 92/100\n",
      "104/137 [=====================>........] - ETA: 0s - loss: 0.3107 - recall: 0.9159\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3189 - recall: 0.9097 - val_loss: 0.3981 - val_recall: 0.6875\n",
      "Epoch 93/100\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.3266 - recall: 0.9371\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3261 - recall: 0.9375 - val_loss: 0.3914 - val_recall: 0.6875\n",
      "Epoch 94/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3290 - recall: 0.8819\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3290 - recall: 0.8819 - val_loss: 0.3921 - val_recall: 0.6875\n",
      "Epoch 95/100\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.3102 - recall: 0.9306\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3102 - recall: 0.9306 - val_loss: 0.4020 - val_recall: 0.6875\n",
      "Epoch 96/100\n",
      "107/137 [======================>.......] - ETA: 0s - loss: 0.3313 - recall: 0.9035\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3251 - recall: 0.9236 - val_loss: 0.3950 - val_recall: 0.6875\n",
      "Epoch 97/100\n",
      "105/137 [=====================>........] - ETA: 0s - loss: 0.3378 - recall: 0.8919\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3319 - recall: 0.9028 - val_loss: 0.4004 - val_recall: 0.6875\n",
      "Epoch 98/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3218 - recall: 0.8944\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3225 - recall: 0.8958 - val_loss: 0.3881 - val_recall: 0.6562\n",
      "Epoch 99/100\n",
      "102/137 [=====================>........] - ETA: 0s - loss: 0.3102 - recall: 0.9159\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3177 - recall: 0.9097 - val_loss: 0.3935 - val_recall: 0.6562\n",
      "Epoch 100/100\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.3149 - recall: 0.9225\n",
      "best recall is : 0.875\n",
      "137/137 [==============================] - 0s 2ms/step - loss: 0.3156 - recall: 0.9236 - val_loss: 0.3872 - val_recall: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b24302a8b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.2, batch_size=32, epochs=100, callbacks=callbacks, class_weight=class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[736, 584],\n",
       "       [  4,  40]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('bestmodel.hdf5')\n",
    "confusion_matrix(y_test, np.round(model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
